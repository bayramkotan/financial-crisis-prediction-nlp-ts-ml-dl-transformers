{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9ovIVV89PcI"
      },
      "source": [
        "#  NLP 2018 - Phase 1: Veri Hazırlama\n",
        "\n",
        "**Bu notebook'u sadece bir kez çalıştır!**\n",
        "\n",
        "İşlemler:\n",
        "1. Haber verisi çekme/yükleme\n",
        "2. Türkçe → İngilizce çeviri\n",
        "3. Lemmatization & Stemming\n",
        "\n",
        "Çıktılar `/content/drive/MyDrive/Colab Notebooks/DataFrames/` klasörüne kaydedilir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6iZEQ1h0Nto",
        "outputId": "f76084cf-a4ab-41da-dd07-2c4e4fcafcf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Bunu çalıştırdıktan sonra 'Restart Session' a bas\n",
        "!pip install -q sentence-transformers deep_translator gensim fasttext beautifulsoup4 inflect requests pandas openpyxl tensorflow tensorflow-hub"
      ],
      "metadata": {
        "id": "nwgIRas20ba_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veri İşleme ve Analiz\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import warnings\n",
        "\n",
        "# Metin Ön İşleme\n",
        "from bs4 import BeautifulSoup\n",
        "import inflect\n",
        "from deep_translator import GoogleTranslator\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# NLP Modelleri\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader as api\n",
        "import tensorflow_hub as hub\n",
        "import fasttext\n",
        "import os\n",
        "\n",
        "# Uyarıları gizle\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NDWvc-_R0wPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "\n",
        "### NLTK PAKETLERINI DRIVE'A KALICI İNDİRME\n",
        "\n",
        "DRIVE_NLTK_PATH = \"/content/drive/MyDrive/Colab Notebooks/NLP Modeller/NLTK\"\n",
        "NLTK_PACKAGES = [\n",
        "    'punkt',\n",
        "    'punkt_tab',\n",
        "    'stopwords',\n",
        "    'averaged_perceptron_tagger',\n",
        "    'averaged_perceptron_tagger_eng',  # YENİ! ← NLTK 3.9+ için gerekli\n",
        "    'wordnet',\n",
        "    'omw-1.4'\n",
        "]\n",
        "\n",
        "os.makedirs(DRIVE_NLTK_PATH, exist_ok=True)\n",
        "\n",
        "if DRIVE_NLTK_PATH not in nltk.data.path:\n",
        "    nltk.data.path.insert(0, DRIVE_NLTK_PATH)\n",
        "\n",
        "print(\" NLTK paketleri kontrol ediliyor...\\n\")\n",
        "\n",
        "for package in NLTK_PACKAGES:\n",
        "    try:\n",
        "        package_exists = False\n",
        "        possible_paths = [\n",
        "            os.path.join(DRIVE_NLTK_PATH, 'corpora', package),\n",
        "            os.path.join(DRIVE_NLTK_PATH, 'tokenizers', package),\n",
        "            os.path.join(DRIVE_NLTK_PATH, 'taggers', package),\n",
        "        ]\n",
        "\n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                package_exists = True\n",
        "                print(f\"   {package:40s} → Drive'da mevcut\")\n",
        "                break\n",
        "\n",
        "        if not package_exists:\n",
        "            print(f\"   {package:40s} → İndiriliyor...\")\n",
        "            nltk.download(package, download_dir=DRIVE_NLTK_PATH, quiet=True)\n",
        "            print(f\"   {package:40s} → Drive'a indirildi!\")\n",
        "    except Exception as e:\n",
        "        print(f\"   {package:40s} → HATA: {e}\")\n",
        "\n",
        "print(\"\\n NLTK kurulumu tamamlandı!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0gz-pkJBwwa",
        "outputId": "04f2832a-432b-4a35-931a-c8376ab66fad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sayıları yazıya çevirmek için inflect motorunu başlat\n",
        "p = inflect.engine()\n",
        "\n",
        "def preprocess_text(text, remove_stop_words=False):\n",
        "    # Girdinin bir metin (string) olup olmadığını kontrol et, değilse boş metin döndür\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # HTML etiketlerini temizle\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # Sayıları kelimelere çevir\n",
        "    # Not: Büyük metinlerde bu işlem yavaş olabilir.\n",
        "    def replace_number(match):\n",
        "        number = int(match.group(0))\n",
        "        return p.number_to_words(number)\n",
        "    text = re.sub(r'\\d+', replace_number, text)\n",
        "\n",
        "\n",
        "    # Türkçe karakterleri koruyarak harf dışı karakterleri temizle\n",
        "    text = re.sub('[^a-zA-ZğĞçÇşŞüÜöÖıİ]', ' ', text)\n",
        "    text = text.lower()\n",
        "    words = text.split()\n",
        "\n",
        "    # İsteğe bağlı olarak stop-words (etkisiz kelimeler) kaldırma\n",
        "    if remove_stop_words:\n",
        "        turkish_stopwords = set(stopwords.words('turkish'))\n",
        "        words = [w for w in words if not w in turkish_stopwords]\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "9_4RrWcC0ykK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Gerekli kütüphaneyi yükleyin\n",
        "!pip install newsapi-python"
      ],
      "metadata": {
        "id": "8gVfy2dd7Mz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerekli kütüphaneyi yükleyin\n",
        "!pip install gnews\n",
        "\n",
        "from gnews import GNews\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "\n",
        "# --- BAŞLANGIÇ AYARLARI ---\n",
        "# Bu yöntemde API anahtarına gerek YOKTUR.\n",
        "# Eski tarihleri sorgulayabilirsiniz.\n",
        "start_date_str = '2018-01-01'\n",
        "end_date_str = '2018-12-31'\n",
        "\n",
        "# Tarihleri datetime nesnesine dönüştürme\n",
        "start_date_dt = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
        "end_date_dt = datetime.strptime(end_date_str, '%Y-%m-%d')\n",
        "\n",
        "# Aranacak anahtar kelimeler\n",
        "keywords = [\n",
        "    \"borsa\", \"hisse senedi\", \"piyasa\", \"yatırım\", \"ekonomi\", \"ticaret\",\n",
        "    \"finans\", \"para piyasası\", \"sermaye piyasası\", \"döviz\", \"endeks\"\n",
        "]\n",
        "# --- BİTİŞ AYARLARI ---\n",
        "\n",
        "# --- DÜZELTME BURADA ---\n",
        "# GNews nesnesini başlatma (Önce boş başlatıp, sonra özelliklerini atamak daha garantilidir)\n",
        "google_news = GNews()\n",
        "google_news.language = 'tr'\n",
        "google_news.country = 'TR'\n",
        "# --- DÜZELTME BİTTİ ---\n",
        "\n",
        "# Tarih aralığını ayarla\n",
        "google_news.start_date = (start_date_dt.year, start_date_dt.month, start_date_dt.day)\n",
        "google_news.end_date = (end_date_dt.year, end_date_dt.month, end_date_dt.day)\n",
        "\n",
        "all_entries = []\n",
        "\n",
        "try:\n",
        "    # Her bir anahtar kelime için ayrı ayrı arama yapıp sonuçları birleştirebiliriz\n",
        "    print(\"Haberler çekiliyor, bu işlem biraz zaman alabilir...\")\n",
        "    for keyword in keywords:\n",
        "        json_resp = google_news.get_news(keyword)\n",
        "        for item in json_resp:\n",
        "            entry = {\n",
        "                'Tarih': item.get('published date'),\n",
        "                'Başlık': item.get('title'),\n",
        "                'Metin': item.get('description')\n",
        "            }\n",
        "            all_entries.append(entry)\n",
        "\n",
        "    # Tekrarlanan haberleri başlıklarına göre temizle\n",
        "    if all_entries:\n",
        "        temp_df = pd.DataFrame(all_entries)\n",
        "        temp_df.drop_duplicates(subset=['Başlık'], inplace=True)\n",
        "        all_entries = temp_df.to_dict('records')\n",
        "\n",
        "    print(f\"Toplam {len(all_entries)} adet benzersiz haber bulundu.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Veri çekilirken bir hata oluştu: {e}\")\n",
        "\n",
        "\n",
        "# Verileri bir pandas DataFrame'ine çevirme ve kaydetme\n",
        "if all_entries:\n",
        "    df = pd.DataFrame(all_entries)\n",
        "    csv_file_name = f'/content/drive/MyDrive/Colab Notebooks/DataFrames/BorsaHaberleri_Neutr_{start_date_str}_{end_date_str}.csv'\n",
        "    df.to_csv(csv_file_name, index=False)\n",
        "    print(f\"Veriler '{csv_file_name}' dosyasına başarıyla kaydedildi.\")\n",
        "else:\n",
        "    print(\"Belirtilen kriterlerde hiç haber bulunamadı.\")\n",
        "    df = pd.DataFrame() # Boş DataFrame oluştur"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLin--aM01De",
        "outputId": "81419e1a-1a1e-4138-ede7-e004f65de988"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eğer bir önceki hücrede veri çekildiyse bu hücreyi çalıştırın\n",
        "# Eğer daha önce çektiğiniz bir dosyayı kullanacaksanız, dosya adını güncelleyin.\n",
        "start_date_str = '2018-01-01'\n",
        "end_date_str = '2018-12-31'\n",
        "\n",
        "csv_file_name = f'/content/drive/MyDrive/Colab Notebooks/DataFrames/BorsaHaberleri_Neutr_{start_date_str}_{end_date_str}.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(csv_file_name)\n",
        "    # Metin sütununa ön işleme uygula\n",
        "    df['Metin'] = df['Metin'].apply(preprocess_text)\n",
        "    print(\"Veri başarıyla yüklendi ve ön işleme tamamlandı.\")\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Hata: '{csv_file_name}' dosyası bulunamadı. Lütfen dosya yolunu kontrol edin veya bir önceki hücreyi çalıştırarak veriyi indirin.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "CbQ1I8Xs53SA",
        "outputId": "906fbd11-5cf6-421c-cd7f-a2643f2111ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC7TqP-2ufRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "377c79e8-e74f-476d-ec16-ba8e67b187b5"
      },
      "outputs": [],
      "source": [
        "# Haber verisi özeti\n",
        "if 'df' in locals() and not df.empty:\n",
        "    print(f\"\\n Toplam haber: {len(df):,}\")\n",
        "    print(f\" Sütunlar: {', '.join(df.columns.tolist())}\")\n",
        "    print(\"\\n İlk 3 haber:\")\n",
        "    for i in range(min(3, len(df))):\n",
        "        if 'Baslik' in df.columns:\n",
        "            print(f\"  {i+1}. {df['Baslik'].iloc[i][:70]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_to_english(text):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"\"\n",
        "    translated_text = \"\"\n",
        "    # Metni 4900 karakterlik parçalara böl (Google Translate API limiti ~5000)\n",
        "    text_parts = [text[i:i + 4900] for i in range(0, len(text), 4900)]\n",
        "\n",
        "    for part in text_parts:\n",
        "        try:\n",
        "            # Gecikme ekleyerek API'ye çok sık istek atmayı önle\n",
        "            # time.sleep(0.5)\n",
        "            translated_text += GoogleTranslator(source='tr', target='en').translate(part)\n",
        "        except Exception as e:\n",
        "            print(f\"Çeviri hatası: {e}. Orijinal parça kullanılıyor.\")\n",
        "            translated_text += part # Hata durumunda orijinal metni ekle\n",
        "    return translated_text\n",
        "\n",
        "# DataFrame'in 'Metin' sütununu çevir ve yeni bir sütuna ata\n",
        "if not df.empty:\n",
        "    df['english_text'] = df['Metin'].apply(translate_to_english)\n",
        "\n",
        "    # Çevrilmiş veriyi yeni bir CSV olarak kaydet\n",
        "    ingilizce_csv_path = f'/content/drive/MyDrive/Colab Notebooks/DataFrames/BorsaHaberleri_Neutr_{start_date_str}_{end_date_str}-ingilizce.csv'\n",
        "    df.to_csv(ingilizce_csv_path, index=False)\n",
        "    print(f\"Çevrilen metinler '{ingilizce_csv_path}' dosyasına kaydedildi.\")\n",
        "    display(df[['Metin', 'english_text']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "RzwmiYd1-MjE",
        "outputId": "f4c0d6d3-9eb0-41ab-9ab6-bac26316c9fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSVcN8n1ufR0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "b4f98331-90f1-4e27-9512-b48f027eaee9"
      },
      "outputs": [],
      "source": [
        "# Çeviri analizi\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if 'df' in locals() and not df.empty and 'english_text' in df.columns:\n",
        "    df['tr_words'] = df['Metin'].astype(str).apply(lambda x: len(x.split()))\n",
        "    df['en_words'] = df['english_text'].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "    print(f\"\\n Ortalama kelime sayısı:\")\n",
        "    print(f\"  Türkçe: {df['tr_words'].mean():.1f}\")\n",
        "    print(f\"  İngilizce: {df['en_words'].mean():.1f}\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.scatter(df['tr_words'], df['en_words'], alpha=0.5)\n",
        "    ax.plot([0, df['tr_words'].max()], [0, df['tr_words'].max()], 'r--', label='1:1')\n",
        "    ax.set_xlabel('Türkçe Kelime', fontweight='bold')\n",
        "    ax.set_ylabel('İngilizce Kelime', fontweight='bold')\n",
        "    ax.set_title('Çeviri Sonrası Kelime Karşılaştırması', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# NLTK'nın ihtiyaç duyduğu tüm paketleri indiriyoruz.\n",
        "# Hata mesajında belirtilen eksik 'punkt_tab' paketi eklendi.\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True) # <-- EKLENDİ\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "print(\" Gerekli NLTK paketleri indirildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehnDBj7WB41D",
        "outputId": "9f0521ed-f658-4a9d-d11f-502530635d67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# İngilizce veriyi içeren CSV dosyasını yükle\n",
        "start_date_str = '2018-01-01'\n",
        "end_date_str = '2018-12-31'\n",
        "ingilizce_csv_path = f'/content/drive/MyDrive/Colab Notebooks/DataFrames/BorsaHaberleri_Neutr_{start_date_str}_{end_date_str}-ingilizce.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(ingilizce_csv_path)\n",
        "    # Boş (NaN) satırları temizle\n",
        "    df.dropna(subset=['english_text'], inplace=True)\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # Lemmatization için POS-Tag (Kelime Türü) belirleme fonksiyonu\n",
        "    def get_wordnet_pos(word):\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "    # Lemmatization fonksiyonu\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    def lemmatize_text(text):\n",
        "        if not isinstance(text, str): return \"\"\n",
        "        tokens = nltk.word_tokenize(text.lower())\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens if w.isalpha() and w not in stop_words]\n",
        "        return ' '.join(lemmatized_tokens)\n",
        "\n",
        "    # Stemming fonksiyonu\n",
        "    stemmer = PorterStemmer()\n",
        "    def stem_text(text):\n",
        "        if not isinstance(text, str): return \"\"\n",
        "        tokens = nltk.word_tokenize(text.lower())\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        stemmed_tokens = [stemmer.stem(w) for w in tokens if w.isalpha() and w not in stop_words]\n",
        "        return ' '.join(stemmed_tokens)\n",
        "\n",
        "    # Lemmatization ve Stemming işlemlerini uygula\n",
        "    df['lemmatized_text'] = df['english_text'].apply(lemmatize_text)\n",
        "    df['stemmed_text'] = df['english_text'].apply(stem_text)\n",
        "\n",
        "    # Sonuçları yeni bir CSV dosyasına kaydet\n",
        "    kok_bulunmus_csv_path = f'/content/drive/MyDrive/Colab Notebooks/DataFrames/BorsaHaberleri_Neutr_{start_date_str}_{end_date_str}-ingilizce-KokleriBulunmus.csv'\n",
        "    df.to_csv(kok_bulunmus_csv_path, index=False)\n",
        "\n",
        "    print(\"Lemmatization ve Stemming işlemleri tamamlandı ve sonuçlar kaydedildi.\")\n",
        "    display(df[['english_text', 'lemmatized_text', 'stemmed_text']].head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Hata: '{ingilizce_csv_path}' dosyası bulunamadı. Lütfen bir önceki hücreyi çalıştırdığınızdan emin olun.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "YgYRsOlx58aR",
        "outputId": "ae49fc48-b04e-4546-e1a1-ba3f485fd8ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pci3dfKHufR1"
      },
      "outputs": [],
      "source": [
        "# Ön işleme analizi\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if 'df_final' in locals() and not df_final.empty:\n",
        "    df_final['eng_w'] = df_final['english_text'].astype(str).apply(lambda x: len(x.split()))\n",
        "    df_final['lem_w'] = df_final['lemmatized_text'].astype(str).apply(lambda x: len(x.split()))\n",
        "    df_final['stem_w'] = df_final['stemmed_text'].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "    means = [df_final['eng_w'].mean(), df_final['lem_w'].mean(), df_final['stem_w'].mean()]\n",
        "\n",
        "    print(f\"\\n Ortalama kelime sayıları:\")\n",
        "    print(f\"  English: {means[0]:.1f}\")\n",
        "    print(f\"  Lemmatized: {means[1]:.1f}\")\n",
        "    print(f\"  Stemmed: {means[2]:.1f}\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bars = ax.bar(['English', 'Lemmatized', 'Stemmed'], means,\n",
        "                   color=['steelblue', 'orange', 'green'], edgecolor='black')\n",
        "    for bar in bars:\n",
        "        h = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., h, f'{h:.1f}',\n",
        "               ha='center', va='bottom', fontweight='bold')\n",
        "    ax.set_ylabel('Ortalama Kelime', fontweight='bold')\n",
        "    ax.set_title('Ön İşleme Etkisi', fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPbER2FaufR1"
      },
      "outputs": [],
      "source": [
        "# Veri analizi grafikleri\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "if 'df_final' in locals() and not df_final.empty:\n",
        "    # Aylık dağılım\n",
        "    if 'Tarih' in df_final.columns:\n",
        "        df_final['Tarih'] = pd.to_datetime(df_final['Tarih'], errors='coerce')\n",
        "        df_final['Ay'] = df_final['Tarih'].dt.to_period('M')\n",
        "        monthly = df_final['Ay'].value_counts().sort_index()\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 5))\n",
        "        monthly.plot(kind='bar', ax=ax, color='steelblue', edgecolor='black')\n",
        "        ax.set_title('Aylık Haber Dağılımı', fontweight='bold')\n",
        "        ax.set_xlabel('Ay', fontweight='bold')\n",
        "        ax.set_ylabel('Haber Sayısı', fontweight='bold')\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "        for i, v in enumerate(monthly):\n",
        "            ax.text(i, v+2, str(v), ha='center', fontweight='bold')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    }
  ]
}